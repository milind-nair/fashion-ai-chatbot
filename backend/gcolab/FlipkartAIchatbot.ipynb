{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#NLP\n",
        "!pip install langchain==0.0.191 chromadb==0.3.22 llama-cpp-python==0.1.66 \\\n",
        "pdfminer.six==20221105 InstructorEmbedding sentence-transformers faiss-cpu \\\n",
        "huggingface_hub transformers protobuf==3.20.0; sys_platform != 'darwin' protobuf==3.20.0; sys_platform == 'darwin' and platform_machine != 'arm64' \\\n",
        "protobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64' auto-gptq==0.2.2 docx2txt unstructured\n",
        "# Utilities\n",
        "!pip install urllib3==1.26.6 accelerate bitsandbytes ; sys_platform != 'win32' bitsandbytes-windows ; sys_platform == 'win32' click flask requests\n",
        "# Streamlit related\n",
        "!pip install streamlit Streamlit-extras\n",
        "# Excel File Manipulation\n",
        "!pip install openpyxl\n"
      ],
      "metadata": {
        "id": "UPP6d95QDNzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4789ca65-cda7-4544-c5c8-1bfdf1bc1bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.191\n",
            "  Downloading langchain-0.0.191-py3-none-any.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb==0.3.22\n",
            "  Downloading chromadb-0.3.22-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python==0.1.66\n",
            "  Downloading llama_cpp_python-0.1.66.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: line 1: sys_platform: command not found\n",
            "/bin/bash: line 1: sys_platform: command not found\n",
            "/bin/bash: line 1: sys_platform: command not found\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m/bin/bash: line 1: sys_platform: command not found\n",
            "/bin/bash: line 1: sys_platform: command not found\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Streamlit-extras\n",
            "  Downloading streamlit_extras-0.3.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.6)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<10,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.18 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.5.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.7.1)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.21.2-py3-none-any.whl (25 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8 (from streamlit)\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.1)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting htbuilder==0.6.1 (from Streamlit-extras)\n",
            "  Downloading htbuilder-0.6.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 230, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 631, in _prepare_linked_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 69, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/distributions/sdist.py\", line 61, in prepare_distribution_metadata\n",
            "    self.req.prepare_metadata()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 561, in prepare_metadata\n",
            "    self.metadata_directory = generate_metadata_legacy(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/build/metadata_legacy.py\", line 64, in generate_metadata\n",
            "    call_subprocess(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
            "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1701, in print\n",
            "    extend(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/segment.py\", line 198, in <genexpr>\n",
            "    result_segments = (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1330, in render\n",
            "    for render_output in iter_render:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 662, in __rich_console__\n",
            "    all_lines = Text(\"\\n\").join(lines)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 746, in join\n",
            "    new_text = self.blank_copy()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 412, in blank_copy\n",
            "    copy_self = Text(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 127, in __init__\n",
            "    def __init__(\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in _compute_dependencies\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in <listcomp>\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3124, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 304, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 229, in _evaluate_markers\n",
            "    groups[-1].append(_evaluate_markers(marker, environment))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 226, in _evaluate_markers\n",
            "    assert isinstance(marker, (list, tuple, str))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir DB"
      ],
      "metadata": {
        "id": "svYGAMIEHmnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir SOURCE_DOCUMENTS"
      ],
      "metadata": {
        "id": "x3Gb2JzwHqbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel\n",
        "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
        "\n",
        "# load_dotenv()\n",
        "# ROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n",
        "ROOT_DIRECTORY = \"/content\"\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
        "\n",
        "# Can be changed to a specific number\n",
        "INGEST_THREADS = os.cpu_count() or 8\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl=\"duckdb+parquet\", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False\n",
        ")\n",
        "\n",
        "# https://python.langchain.com/en/latest/_modules/langchain/document_loaders/excel.html#UnstructuredExcelLoader\n",
        "DOCUMENT_MAP = {\n",
        "    \".txt\": TextLoader,\n",
        "    \".md\": TextLoader,\n",
        "    \".py\": TextLoader,\n",
        "    \".pdf\": PDFMinerLoader,\n",
        "    \".csv\": CSVLoader,\n",
        "    \".xls\": UnstructuredExcelLoader,\n",
        "    \".xlsx\": UnstructuredExcelLoader,\n",
        "    \".docx\": Docx2txtLoader,\n",
        "    \".doc\": Docx2txtLoader,\n",
        "}\n",
        "\n",
        "# Default Instructor Model\n",
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
        "# You can also choose a smaller model, don't forget to change HuggingFaceInstructEmbeddings\n",
        "# to HuggingFaceEmbeddings in both ingest.py and run_localGPT.py\n",
        "# EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# Select the Model ID and model_basename\n",
        "# load the LLM for generating Natural Language responses\n",
        "\n",
        "# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "# MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
        "\n",
        "# for HF models\n",
        "# MODEL_ID = \"TheBloke/vicuna-7B-1.1-HF\"\n",
        "# MODEL_BASENAME = None\n",
        "# MODEL_ID = \"TheBloke/Wizard-Vicuna-7B-Uncensored-HF\"\n",
        "# MODEL_ID = \"TheBloke/guanaco-7B-HF\"\n",
        "# MODEL_ID = 'NousResearch/Nous-Hermes-13b' # Requires ~ 23GB VRAM. Using STransformers\n",
        "# alongside will 100% create OOM on 24GB cards.\n",
        "# llm = load_model(device_type, model_id=model_id)\n",
        "\n",
        "# for GPTQ (quantized) models\n",
        "# MODEL_ID = \"TheBloke/Nous-Hermes-13B-GPTQ\"\n",
        "# MODEL_BASENAME = \"nous-hermes-13b-GPTQ-4bit-128g.no-act.order\"\n",
        "# MODEL_ID = \"TheBloke/WizardLM-30B-Uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors\" # Requires\n",
        "# ~21GB VRAM. Using STransformers alongside can potentially create OOM on 24GB cards.\n",
        "# MODEL_ID = \"TheBloke/wizardLM-7B-GPTQ\"\n",
        "# MODEL_BASENAME = \"wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors\"\n",
        "MODEL_ID = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "MODEL_BASENAME = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "\n",
        "# for GGML (quantized cpu+gpu+mps) models - check if they support llama.cpp\n",
        "# MODEL_ID = \"TheBloke/wizard-vicuna-13B-GGML\"\n",
        "# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q4_0.bin\"\n",
        "# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q6_K.bin\"\n",
        "# MODEL_BASENAME = \"wizard-vicuna-13B.ggmlv3.q2_K.bin\"\n",
        "# MODEL_ID = \"TheBloke/orca_mini_3B-GGML\"\n",
        "# MODEL_BASENAME = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BHKhAI6YFBTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "\n",
        "import click\n",
        "import torch\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# from constants import (\n",
        "#     CHROMA_SETTINGS,\n",
        "#     DOCUMENT_MAP,\n",
        "#     EMBEDDING_MODEL_NAME,\n",
        "#     INGEST_THREADS,\n",
        "#     PERSIST_DIRECTORY,\n",
        "#     SOURCE_DIRECTORY,\n",
        "# )\n",
        "\n",
        "\n",
        "def load_single_document(file_path: str) -> Document:\n",
        "    # Loads a single document from a file path\n",
        "    file_extension = os.path.splitext(file_path)[1]\n",
        "    loader_class = DOCUMENT_MAP.get(file_extension)\n",
        "    if loader_class:\n",
        "        loader = loader_class(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Document type is undefined\")\n",
        "    return loader.load()[0]\n",
        "\n",
        "\n",
        "def load_document_batch(filepaths):\n",
        "    logging.info(\"Loading document batch\")\n",
        "    # create a thread pool\n",
        "    with ThreadPoolExecutor(len(filepaths)) as exe:\n",
        "        # load files\n",
        "        futures = [exe.submit(load_single_document, name) for name in filepaths]\n",
        "        # collect data\n",
        "        data_list = [future.result() for future in futures]\n",
        "        # return data and file paths\n",
        "        return (data_list, filepaths)\n",
        "\n",
        "\n",
        "# def load_documents(source_dir: str) -> list[Document]:\n",
        "#     # Loads all documents from the source documents directory\n",
        "#     all_files = os.listdir(source_dir)\n",
        "#     paths = []\n",
        "#     for file_path in all_files:\n",
        "#         file_extension = os.path.splitext(file_path)[1]\n",
        "#         source_file_path = os.path.join(source_dir, file_path)\n",
        "#         if file_extension in DOCUMENT_MAP.keys():\n",
        "#             paths.append(source_file_path)\n",
        "\n",
        "#     # Have at least one worker and at most INGEST_THREADS workers\n",
        "#     n_workers = min(INGEST_THREADS, max(len(paths), 1))\n",
        "#     chunksize = round(len(paths) / n_workers)\n",
        "#     docs = []\n",
        "#     with ProcessPoolExecutor(n_workers) as executor:\n",
        "#         futures = []\n",
        "#         # split the load operations into chunks\n",
        "#         for i in range(0, len(paths), chunksize):\n",
        "#             # select a chunk of filenames\n",
        "#             filepaths = paths[i : (i + chunksize)]\n",
        "#             # submit the task\n",
        "#             future = executor.submit(load_document_batch, filepaths)\n",
        "#             futures.append(future)\n",
        "#         # process all results\n",
        "#         for future in as_completed(futures):\n",
        "#             # open the file and load the data\n",
        "#             contents, _ = future.result()\n",
        "#             docs.extend(contents)\n",
        "\n",
        "#     return docs\n",
        "\n",
        "def load_documents(source_dir: str) -> list[Document]:\n",
        "    # Loads all documents from the source documents directory\n",
        "    all_files = os.listdir(source_dir)\n",
        "    paths = []\n",
        "    for file_path in all_files:\n",
        "        file_extension = os.path.splitext(file_path)[1]\n",
        "        source_file_path = os.path.join(source_dir, file_path)\n",
        "        if file_extension in DOCUMENT_MAP.keys():\n",
        "            paths.append(source_file_path)\n",
        "\n",
        "    # Have at least one worker and at most INGEST_THREADS workers\n",
        "    n_workers = min(INGEST_THREADS, max(len(paths), 1))\n",
        "    chunksize = max(round(len(paths) / n_workers), 1)  # Ensure chunksize is not zero\n",
        "    docs = []\n",
        "    with ProcessPoolExecutor(n_workers) as executor:\n",
        "        futures = []\n",
        "        # split the load operations into chunks\n",
        "        for i in range(0, len(paths), chunksize):\n",
        "            # select a chunk of filenames\n",
        "            filepaths = paths[i : (i + chunksize)]\n",
        "            # submit the task\n",
        "            future = executor.submit(load_document_batch, filepaths)\n",
        "            futures.append(future)\n",
        "        # process all results\n",
        "        for future in as_completed(futures):\n",
        "            # open the file and load the data\n",
        "            contents, _ = future.result()\n",
        "            docs.extend(contents)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def split_documents(documents: list[Document]) -> tuple[list[Document], list[Document]]:\n",
        "    # Splits documents for correct Text Splitter\n",
        "    text_docs, python_docs = [], []\n",
        "    for doc in documents:\n",
        "        file_extension = os.path.splitext(doc.metadata[\"source\"])[1]\n",
        "        if file_extension == \".py\":\n",
        "            python_docs.append(doc)\n",
        "        else:\n",
        "            text_docs.append(doc)\n",
        "\n",
        "    return text_docs, python_docs\n"
      ],
      "metadata": {
        "id": "OW4UnuqnH3CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load documents and split in chunks\n",
        "    logging.info(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
        "    documents = load_documents(SOURCE_DIRECTORY)\n",
        "    text_documents, python_documents = split_documents(documents)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "        language=Language.PYTHON, chunk_size=1000, chunk_overlap=200\n",
        "    )\n",
        "    texts = text_splitter.split_documents(text_documents)\n",
        "    texts.extend(python_splitter.split_documents(python_documents))\n",
        "    logging.info(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n",
        "    logging.info(f\"Split into {len(texts)} chunks of text\")\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        model_kwargs={\"device\": \"cuda\"},  # Set device type to \"tpu\"\n",
        "    )\n",
        "    # change the embedding type here if you are running into issues.\n",
        "    # These are much smaller embeddings and will work for most applications\n",
        "    # If you use HuggingFaceEmbeddings, make sure to also use the same in the\n",
        "    # run_localGPT.py file.\n",
        "\n",
        "    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        texts,\n",
        "        embeddings,\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        client_settings=CHROMA_SETTINGS,\n",
        "    )\n",
        "    db.persist()\n",
        "    db = None\n",
        "\n",
        "# Set the device type to TPU and call the main function\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz961E-DJ_if",
        "outputId": "c18c9e10-9be7-4b88-a081-d8cd80933fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/DB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install auto_gptq"
      ],
      "metadata": {
        "id": "43RkyhofOMdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import click\n",
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline, LlamaCpp\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "# from constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME\n",
        "\n",
        "\n",
        "def load_model(device_type, model_id, model_basename=None):\n",
        "    \"\"\"\n",
        "    Select a model for text generation using the HuggingFace library.\n",
        "    If you are running this for the first time, it will download a model for you.\n",
        "    subsequent runs will use the model from the disk.\n",
        "\n",
        "    Args:\n",
        "        device_type (str): Type of device to use, e.g., \"cuda\" for GPU or \"cpu\" for CPU.\n",
        "        model_id (str): Identifier of the model to load from HuggingFace's model hub.\n",
        "        model_basename (str, optional): Basename of the model if using quantized models.\n",
        "            Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        HuggingFacePipeline: A pipeline object for text generation using the loaded model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported model or device type is provided.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n",
        "    logging.info(\"This action can take a few minutes!\")\n",
        "\n",
        "    if model_basename is not None:\n",
        "        if \".ggml\" in model_basename:\n",
        "            logging.info(\"Using Llamacpp for GGML quantized models\")\n",
        "            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)\n",
        "            max_ctx_size = 2048\n",
        "            kwargs = {\n",
        "                \"model_path\": model_path,\n",
        "                \"n_ctx\": max_ctx_size,\n",
        "                \"max_tokens\": max_ctx_size,\n",
        "            }\n",
        "            if device_type.lower() == \"mps\":\n",
        "                kwargs[\"n_gpu_layers\"] = 1000\n",
        "            if device_type.lower() == \"cuda\":\n",
        "                kwargs[\"n_gpu_layers\"] = 1000\n",
        "                kwargs[\"n_batch\"] = max_ctx_size\n",
        "            return LlamaCpp(**kwargs)\n",
        "\n",
        "        else:\n",
        "            # The code supports all huggingface models that ends with GPTQ and have some variation\n",
        "            # of .no-act.order or .safetensors in their HF repo.\n",
        "            logging.info(\"Using AutoGPTQForCausalLM for quantized models\")\n",
        "\n",
        "            if \".safetensors\" in model_basename:\n",
        "                # Remove the \".safetensors\" ending if present\n",
        "                model_basename = model_basename.replace(\".safetensors\", \"\")\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "            logging.info(\"Tokenizer loaded\")\n",
        "\n",
        "            model = AutoGPTQForCausalLM.from_quantized(\n",
        "                model_id,\n",
        "                model_basename=model_basename,\n",
        "                use_safetensors=True,\n",
        "                trust_remote_code=True,\n",
        "                device=\"cuda:0\",\n",
        "                use_triton=False,\n",
        "                quantize_config=None,\n",
        "            )\n",
        "    elif (\n",
        "        device_type.lower() == \"cuda\"\n",
        "    ):  # The code supports all huggingface models that ends with -HF or which have a .bin\n",
        "        # file in their HF repo.\n",
        "        logging.info(\"Using AutoModelForCausalLM for full models\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        logging.info(\"Tokenizer loaded\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "            # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
        "        )\n",
        "        model.tie_weights()\n",
        "    else:\n",
        "        logging.info(\"Using LlamaTokenizer\")\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "        model = LlamaForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "    # Load configuration from the model to avoid warnings\n",
        "    generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "    # see here for details:\n",
        "    # https://huggingface.co/docs/transformers/\n",
        "    # main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=2048,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    logging.info(\"Local LLM Loaded\")\n",
        "\n",
        "    return local_llm\n"
      ],
      "metadata": {
        "id": "LkI52w1TUbh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main(device_type, show_sources):\n",
        "def main2():\n",
        "    \"\"\"\n",
        "    This function implements the information retrieval task.\n",
        "\n",
        "\n",
        "    1. Loads an embedding model, can be HuggingFaceInstructEmbeddings or HuggingFaceEmbeddings\n",
        "    2. Loads the existing vectorestore that was created by inget.py\n",
        "    3. Loads the local LLM using load_model function - You can now set different LLMs.\n",
        "    4. Setup the Question Answer retreival chain.\n",
        "    5. Question answers.\n",
        "    \"\"\"\n",
        "\n",
        "    logging.info(f\"Running on: cuda\")\n",
        "    # logging.info(f\"Display Source Documents set to: {show_sources}\")\n",
        "\n",
        "    embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"})\n",
        "\n",
        "    # uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n",
        "    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "    # load the vectorstore\n",
        "    db = Chroma(\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        embedding_function=embeddings,\n",
        "        client_settings=CHROMA_SETTINGS,\n",
        "    )\n",
        "    retriever = db.as_retriever()\n",
        "\n",
        "\n",
        "    template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
        "    just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    {history}\n",
        "    Question: {question}\n",
        "    Helpful Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\n",
        "    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n",
        "\n",
        "    llm = load_model(\"cuda\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
        "    )\n",
        "    # Interactive questions and answers\n",
        "    # print(\"satvik\")\n",
        "    while True:\n",
        "        query = input(\"\\nEnter a query: \")\n",
        "        if query == \"exit\":\n",
        "            break\n",
        "        # Get the answer from the chain\n",
        "        res = qa(query)\n",
        "        answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "\n",
        "        # Print the result\n",
        "        print(\"\\n\\n> Question:\")\n",
        "        print(query)\n",
        "        print(\"\\n> Answer:\")\n",
        "        print(answer)\n",
        "\n",
        "        # if show_sources:  # this is a flag that you can set to disable showing answers.\n",
        "        #     # # Print the relevant sources used for the answer\n",
        "        #     print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n",
        "        #     for document in docs:\n",
        "        #         print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
        "        #         print(document.page_content)\n",
        "        #     print(\"----------------------------------SOURCE DOCUMENTS---------------------------\")\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     logging.basicConfig(\n",
        "#         format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s\", level=logging.INFO\n",
        "#     )\n",
        "main2()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mM5qrL7UZZI",
        "outputId": "d971ba48-f5ab-4419-9aaf-87993ae011c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n"
          ]
        }
      ]
    }
  ]
}