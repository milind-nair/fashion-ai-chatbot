{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX23245KALWG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1zuUA7NMHAQ"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken 2U1L7h0W1hitgbVVijS1tsdCi0V_2KrJUQ3fibyro8UGqak7V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3f5DfTRMOsf"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UPP6d95QDNzG"
      },
      "outputs": [],
      "source": [
        "#NLP\n",
        "!pip install langchain==0.0.191 chromadb==0.3.22 llama-cpp-python==0.1.66 \\\n",
        "pdfminer.six==20221105 InstructorEmbedding sentence-transformers faiss-cpu \\\n",
        "huggingface_hub transformers protobuf==3.20.0; sys_platform != 'darwin' protobuf==3.20.0; sys_platform == 'darwin' and platform_machine != 'arm64' \\\n",
        "protobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64' auto-gptq==0.2.2 docx2txt unstructured\n",
        "# Utilities\n",
        "!pip install urllib3==1.26.6 accelerate bitsandbytes ; sys_platform != 'win32' bitsandbytes-windows ; sys_platform == 'win32' click flask requests\n",
        "# Streamlit related\n",
        "!pip install streamlit Streamlit-extras\n",
        "# Excel File Manipulation\n",
        "!pip install openpyxl\n",
        "#GPTQ\n",
        "!pip install auto_gptq\n",
        "#PYNGROK\n",
        "!pip install pyngrok\n",
        "\n",
        "#Ngrok config\n",
        "!ngrok authtoken 2U1L7h0W1hitgbVVijS1tsdCi0V_2KrJUQ3fibyro8UGqak7V\n",
        "!pkill -f ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dy4iXQ4ABzh"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svYGAMIEHmnp"
      },
      "outputs": [],
      "source": [
        "!mkdir DB\n",
        "!mkdir SOURCE_DOCUMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHKhAI6YFBTe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel\n",
        "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
        "\n",
        "# load_dotenv()\n",
        "# ROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n",
        "ROOT_DIRECTORY = \"/content\"\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
        "\n",
        "# Can be changed to a specific number\n",
        "INGEST_THREADS = os.cpu_count() or 8\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl=\"duckdb+parquet\", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False\n",
        ")\n",
        "\n",
        "# https://python.langchain.com/en/latest/_modules/langchain/document_loaders/excel.html#UnstructuredExcelLoader\n",
        "DOCUMENT_MAP = {\n",
        "    \".txt\": TextLoader,\n",
        "    \".md\": TextLoader,\n",
        "    \".py\": TextLoader,\n",
        "    \".pdf\": PDFMinerLoader,\n",
        "    \".csv\": CSVLoader,\n",
        "    \".xls\": UnstructuredExcelLoader,\n",
        "    \".xlsx\": UnstructuredExcelLoader,\n",
        "    \".docx\": Docx2txtLoader,\n",
        "    \".doc\": Docx2txtLoader,\n",
        "}\n",
        "\n",
        "# Default Instructor Model\n",
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
        "# You can also choose a smaller model, don't forget to change HuggingFaceInstructEmbeddings\n",
        "# to HuggingFaceEmbeddings in both ingest.py and run_localGPT.py\n",
        "# EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "# Select the Model ID and model_basename\n",
        "# load the LLM for generating Natural Language responses\n",
        "\n",
        "# for GPTQ (quantized) models\n",
        "# MODEL_ID = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "# MODEL_BASENAME = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "\n",
        "# for GGML (quantized cpu+gpu+mps) models - check if they support llama.cpp\n",
        "# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "# MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
        "MODEL_ID = \"TheBloke/orca_mini_3B-GGML\"\n",
        "MODEL_BASENAME = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW4UnuqnH3CV"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "\n",
        "import click\n",
        "import torch\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "def load_single_document(file_path: str) -> Document:\n",
        "    # Loads a single document from a file path\n",
        "    file_extension = os.path.splitext(file_path)[1]\n",
        "    loader_class = DOCUMENT_MAP.get(file_extension)\n",
        "    if loader_class:\n",
        "        loader = loader_class(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Document type is undefined\")\n",
        "    return loader.load()[0]\n",
        "\n",
        "def load_document_batch(filepaths):\n",
        "    logging.info(\"Loading document batch\")\n",
        "    # create a thread pool\n",
        "    with ThreadPoolExecutor(len(filepaths)) as exe:\n",
        "        # load files\n",
        "        futures = [exe.submit(load_single_document, name) for name in filepaths]\n",
        "        # collect data\n",
        "        data_list = [future.result() for future in futures]\n",
        "        # return data and file paths\n",
        "        return (data_list, filepaths)\n",
        "\n",
        "def load_documents(source_dir: str) -> list[Document]:\n",
        "    # Loads all documents from the source documents directory\n",
        "    all_files = os.listdir(source_dir)\n",
        "    paths = []\n",
        "    for file_path in all_files:\n",
        "        file_extension = os.path.splitext(file_path)[1]\n",
        "        source_file_path = os.path.join(source_dir, file_path)\n",
        "        if file_extension in DOCUMENT_MAP.keys():\n",
        "            paths.append(source_file_path)\n",
        "\n",
        "    # Have at least one worker and at most INGEST_THREADS workers\n",
        "    n_workers = min(INGEST_THREADS, max(len(paths), 1))\n",
        "    chunksize = max(round(len(paths) / n_workers), 1)  # Ensure chunksize is not zero\n",
        "    docs = []\n",
        "    with ProcessPoolExecutor(n_workers) as executor:\n",
        "        futures = []\n",
        "        # split the load operations into chunks\n",
        "        for i in range(0, len(paths), chunksize):\n",
        "            # select a chunk of filenames\n",
        "            filepaths = paths[i : (i + chunksize)]\n",
        "            # submit the task\n",
        "            future = executor.submit(load_document_batch, filepaths)\n",
        "            futures.append(future)\n",
        "        # process all results\n",
        "        for future in as_completed(futures):\n",
        "            # open the file and load the data\n",
        "            contents, _ = future.result()\n",
        "            docs.extend(contents)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def split_documents(documents: list[Document]) -> tuple[list[Document], list[Document]]:\n",
        "    # Splits documents for correct Text Splitter\n",
        "    text_docs, python_docs = [], []\n",
        "    for doc in documents:\n",
        "        file_extension = os.path.splitext(doc.metadata[\"source\"])[1]\n",
        "        if file_extension == \".py\":\n",
        "            python_docs.append(doc)\n",
        "        else:\n",
        "            text_docs.append(doc)\n",
        "\n",
        "    return text_docs, python_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz961E-DJ_if"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # Load documents and split in chunks\n",
        "    logging.info(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
        "    documents = load_documents(SOURCE_DIRECTORY)\n",
        "    text_documents, python_documents = split_documents(documents)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "        language=Language.PYTHON, chunk_size=1000, chunk_overlap=200\n",
        "    )\n",
        "    texts = text_splitter.split_documents(text_documents)\n",
        "    texts.extend(python_splitter.split_documents(python_documents))\n",
        "    logging.info(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n",
        "    logging.info(f\"Split into {len(texts)} chunks of text\")\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL_NAME,\n",
        "        model_kwargs={\"device\": \"cpu\"},  # Set device type to \"tpu\"\n",
        "    )\n",
        "    # change the embedding type here if you are running into issues.\n",
        "    # These are much smaller embeddings and will work for most applications\n",
        "    # If you use HuggingFaceEmbeddings, make sure to also use the same in the\n",
        "    # run_localGPT.py file.\n",
        "\n",
        "    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "    db = Chroma.from_documents(\n",
        "        texts,\n",
        "        embeddings,\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        client_settings=CHROMA_SETTINGS,\n",
        "    )\n",
        "    db.persist()\n",
        "    db = None\n",
        "\n",
        "# Set the device type to TPU and call the main function\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkI52w1TUbh2"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "import click\n",
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline, LlamaCpp\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "# from constants import CHROMA_SETTINGS, EMBEDDING_MODEL_NAME, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME\n",
        "\n",
        "\n",
        "def load_model(device_type, model_id, model_basename=None):\n",
        "    \"\"\"\n",
        "    Select a model for text generation using the HuggingFace library.\n",
        "    If you are running this for the first time, it will download a model for you.\n",
        "    subsequent runs will use the model from the disk.\n",
        "\n",
        "    Args:\n",
        "        device_type (str): Type of device to use, e.g., \"cuda\" for GPU or \"cpu\" for CPU.\n",
        "        model_id (str): Identifier of the model to load from HuggingFace's model hub.\n",
        "        model_basename (str, optional): Basename of the model if using quantized models.\n",
        "            Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        HuggingFacePipeline: A pipeline object for text generation using the loaded model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported model or device type is provided.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n",
        "    logging.info(\"This action can take a few minutes!\")\n",
        "\n",
        "    if model_basename is not None:\n",
        "        if \".ggml\" in model_basename:\n",
        "            logging.info(\"Using Llamacpp for GGML quantized models\")\n",
        "            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)\n",
        "            max_ctx_size = 2048\n",
        "            kwargs = {\n",
        "                \"model_path\": model_path,\n",
        "                \"n_ctx\": max_ctx_size,\n",
        "                \"max_tokens\": max_ctx_size,\n",
        "            }\n",
        "            if device_type.lower() == \"mps\":\n",
        "                kwargs[\"n_gpu_layers\"] = 1000\n",
        "            if device_type.lower() == \"cuda\":\n",
        "                kwargs[\"n_gpu_layers\"] = 1000\n",
        "                kwargs[\"n_batch\"] = max_ctx_size\n",
        "            return LlamaCpp(**kwargs)\n",
        "\n",
        "        else:\n",
        "            # The code supports all huggingface models that ends with GPTQ and have some variation\n",
        "            # of .no-act.order or .safetensors in their HF repo.\n",
        "            logging.info(\"Using AutoGPTQForCausalLM for quantized models\")\n",
        "\n",
        "            if \".safetensors\" in model_basename:\n",
        "                # Remove the \".safetensors\" ending if present\n",
        "                model_basename = model_basename.replace(\".safetensors\", \"\")\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "            logging.info(\"Tokenizer loaded\")\n",
        "\n",
        "            model = AutoGPTQForCausalLM.from_quantized(\n",
        "                model_id,\n",
        "                model_basename=model_basename,\n",
        "                use_safetensors=True,\n",
        "                trust_remote_code=True,\n",
        "                device=\"cuda:0\",\n",
        "                use_triton=False,\n",
        "                quantize_config=None,\n",
        "            )\n",
        "    elif (\n",
        "        device_type.lower() == \"cuda\"\n",
        "    ):  # The code supports all huggingface models that ends with -HF or which have a .bin\n",
        "        # file in their HF repo.\n",
        "        logging.info(\"Using AutoModelForCausalLM for full models\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        logging.info(\"Tokenizer loaded\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "            # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
        "        )\n",
        "        model.tie_weights()\n",
        "    else:\n",
        "        logging.info(\"Using LlamaTokenizer\")\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "        model = LlamaForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "    # Load configuration from the model to avoid warnings\n",
        "    generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "    # see here for details:\n",
        "    # https://huggingface.co/docs/transformers/\n",
        "    # main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=2048,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    logging.info(\"Local LLM Loaded\")\n",
        "\n",
        "    return local_llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mM5qrL7UZZI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This function implements the information retrieval task.\n",
        "\n",
        "\n",
        "1. Loads an embedding model, can be HuggingFaceInstructEmbeddings or HuggingFaceEmbeddings\n",
        "2. Loads the existing vectorestore that was created by inget.py\n",
        "3. Loads the local LLM using load_model function - You can now set different LLMs.\n",
        "4. Setup the Question Answer retreival chain.\n",
        "5. Question answers.\n",
        "\"\"\"\n",
        "\n",
        "# logging.info(f\"Running on: cuda\")\n",
        "logging.info(f\"Running on: cpu\")\n",
        "\n",
        "# logging.info(f\"Display Source Documents set to: {show_sources}\")\n",
        "\n",
        "embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cpu\"})\n",
        "# embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"})\n",
        "\n",
        "# load the vectorstore\n",
        "db = Chroma(\n",
        "    persist_directory=PERSIST_DIRECTORY,\n",
        "    embedding_function=embeddings,\n",
        "    client_settings=CHROMA_SETTINGS,\n",
        ")\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
        "just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "{history}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\n",
        "memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n",
        "\n",
        "# llm = load_model(\"cuda\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
        "llm = load_model(\"cpu\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
        ")\n",
        "# Interactive questions and answers\n",
        "\n",
        "# while True:\n",
        "#     # query = input(\"\\nEnter a query: \")\n",
        "#     if query == \"exit\":\n",
        "#         break\n",
        "#     # Get the answer from the chain\n",
        "#     res = qa(query)\n",
        "#     answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "\n",
        "#     # Print the result\n",
        "#     print(\"\\n\\n> Question:\")\n",
        "#     print(query)\n",
        "#     print(\"\\n> Answer:\")\n",
        "#     print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLKRT2vr-7tT"
      },
      "outputs": [],
      "source": [
        "from chromadb.api.types import QueryResult\n",
        "# Define your endpoint for handling prompts\n",
        "@app.route('/prompt', methods=['POST'])\n",
        "def handle_prompt():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        # query = request.form.get('query')\n",
        "\n",
        "        query = data['query']\n",
        "        print(query)\n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Missing 'query' parameter\"}), 400\n",
        "\n",
        "        res = qa(query)\n",
        "        print(\"hello\")\n",
        "        answer= res['result']\n",
        "        print(answer)\n",
        "        # answer=\"satvik\"\n",
        "\n",
        "        return jsonify({\"question\": query, \"answer\": answer})\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(e)\n",
        "        return jsonify({\"error\": e}), 500\n",
        "\n",
        "# Use ngrok to expose the local Flask app to a public URL\n",
        "public_url = ngrok.connect(addr=\"5000\", proto=\"http\")\n",
        "\n",
        "print('Public URL:', public_url)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}