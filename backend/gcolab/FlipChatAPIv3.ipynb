{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UPP6d95QDNzG"
      },
      "outputs": [],
      "source": [
        "# #NLP\n",
        "# !pip install langchain==0.0.191 chromadb==0.3.22 llama-cpp-python==0.1.66 \\\n",
        "# pdfminer.six==20221105 InstructorEmbedding sentence-transformers faiss-cpu \\\n",
        "# huggingface_hub transformers protobuf==3.20.0; sys_platform != 'darwin' protobuf==3.20.0; sys_platform == 'darwin' and platform_machine != 'arm64' \\\n",
        "# protobuf==3.20.3; sys_platform == 'darwin' and platform_machine == 'arm64' auto-gptq==0.2.2 docx2txt unstructured\n",
        "# # Utilities\n",
        "# !pip install urllib3==1.26.6 accelerate bitsandbytes ; sys_platform != 'win32' bitsandbytes-windows ; sys_platform == 'win32' click flask requests\n",
        "# # Streamlit related\n",
        "# !pip install streamlit Streamlit-extras\n",
        "# # Excel File Manipulation\n",
        "# !pip install openpyxl\n",
        "# #GPTQ\n",
        "# !pip install auto_gptq\n",
        "# #PYNGROK\n",
        "# !pip install pyngrok\n",
        "# !pip install fastapi\n",
        "# !pip install uvicorn\n",
        "# !pip install nest-asyncio\n",
        "# #Excel\n",
        "# !pip install xformers\n",
        "# !pip install unstructured\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIPZXVm62iJp",
        "outputId": "e5abc843-442b-4566-9687-198f0837b5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "#Ngrok config\n",
        "!ngrok authtoken 2U1L7h0W1hitgbVVijS1tsdCi0V_2KrJUQ3fibyro8UGqak7V\n",
        "!pkill -f ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-dy4iXQ4ABzh"
      },
      "outputs": [],
      "source": [
        "# from flask import Flask, request, jsonify\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# app = Flask(__name__)\n",
        "app= FastAPI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e5JEdB30gW60"
      },
      "outputs": [],
      "source": [
        "origins=[\"http://127.0.0.1:3000\"]\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "svYGAMIEHmnp"
      },
      "outputs": [],
      "source": [
        "# !mkdir DB\n",
        "# !mkdir SOURCE_DOCUMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BHKhAI6YFBTe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/excel.html?highlight=xlsx#microsoft-excel\n",
        "from langchain.document_loaders import CSVLoader, PDFMinerLoader, TextLoader, UnstructuredExcelLoader, Docx2txtLoader\n",
        "\n",
        "# load_dotenv()\n",
        "# ROOT_DIRECTORY = os.path.dirname(os.path.realpath(__file__))\n",
        "ROOT_DIRECTORY = \"/content\"\n",
        "\n",
        "# Define the folder for storing database\n",
        "SOURCE_DIRECTORY = f\"{ROOT_DIRECTORY}/SOURCE_DOCUMENTS\"\n",
        "\n",
        "PERSIST_DIRECTORY = f\"{ROOT_DIRECTORY}/DB\"\n",
        "\n",
        "# Can be changed to a specific number\n",
        "INGEST_THREADS = os.cpu_count() or 8\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "    chroma_db_impl=\"duckdb+parquet\", persist_directory=PERSIST_DIRECTORY, anonymized_telemetry=False\n",
        ")\n",
        "\n",
        "# https://python.langchain.com/en/latest/_modules/langchain/document_loaders/excel.html#UnstructuredExcelLoader\n",
        "DOCUMENT_MAP = {\n",
        "    \".txt\": TextLoader,\n",
        "    \".md\": TextLoader,\n",
        "    \".py\": TextLoader,\n",
        "    \".pdf\": PDFMinerLoader,\n",
        "    \".csv\": CSVLoader,\n",
        "    \".xls\": UnstructuredExcelLoader,\n",
        "    \".xlsx\": UnstructuredExcelLoader,\n",
        "    \".docx\": Docx2txtLoader,\n",
        "    \".doc\": Docx2txtLoader,\n",
        "}\n",
        "\n",
        "# Default Instructor Model\n",
        "EMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\n",
        "# You can also choose a smaller model, don't forget to change HuggingFaceInstructEmbeddings\n",
        "# to HuggingFaceEmbeddings in both ingest.py and run_localGPT.py\n",
        "# EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "# Select the Model ID and model_basename\n",
        "# load the LLM for generating Natural Language responses\n",
        "\n",
        "# for GPTQ (quantized) models\n",
        "MODEL_ID = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "MODEL_BASENAME = \"WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\"\n",
        "\n",
        "# for GGML (quantized cpu+gpu+mps) models - check if they support llama.cpp\n",
        "# MODEL_ID = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "# MODEL_BASENAME = \"llama-2-7b-chat.ggmlv3.q4_0.bin\"\n",
        "# MODEL_ID = \"TheBloke/orca_mini_3B-GGML\"\n",
        "# MODEL_BASENAME = \"orca-mini-3b.ggmlv3.q4_0.bin\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OW4UnuqnH3CV"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "\n",
        "import click\n",
        "import torch\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "def load_single_document(file_path: str) -> Document:\n",
        "    # Loads a single document from a file path\n",
        "    file_extension = os.path.splitext(file_path)[1]\n",
        "    loader_class = DOCUMENT_MAP.get(file_extension)\n",
        "    if loader_class:\n",
        "        loader = loader_class(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Document type is undefined\")\n",
        "    return loader.load()[0]\n",
        "\n",
        "def load_document_batch(filepaths):\n",
        "    logging.info(\"Loading document batch\")\n",
        "    # create a thread pool\n",
        "    with ThreadPoolExecutor(len(filepaths)) as exe:\n",
        "        # load files\n",
        "        futures = [exe.submit(load_single_document, name) for name in filepaths]\n",
        "        # collect data\n",
        "        data_list = [future.result() for future in futures]\n",
        "        # return data and file paths\n",
        "        return (data_list, filepaths)\n",
        "\n",
        "def load_documents(source_dir: str) -> list[Document]:\n",
        "    # Loads all documents from the source documents directory\n",
        "    all_files = os.listdir(source_dir)\n",
        "    paths = []\n",
        "    for file_path in all_files:\n",
        "        file_extension = os.path.splitext(file_path)[1]\n",
        "        source_file_path = os.path.join(source_dir, file_path)\n",
        "        if file_extension in DOCUMENT_MAP.keys():\n",
        "            paths.append(source_file_path)\n",
        "\n",
        "    # Have at least one worker and at most INGEST_THREADS workers\n",
        "    n_workers = min(INGEST_THREADS, max(len(paths), 1))\n",
        "    chunksize = max(round(len(paths) / n_workers), 1)  # Ensure chunksize is not zero\n",
        "    docs = []\n",
        "    with ProcessPoolExecutor(n_workers) as executor:\n",
        "        futures = []\n",
        "        # split the load operations into chunks\n",
        "        for i in range(0, len(paths), chunksize):\n",
        "            # select a chunk of filenames\n",
        "            filepaths = paths[i : (i + chunksize)]\n",
        "            # submit the task\n",
        "            future = executor.submit(load_document_batch, filepaths)\n",
        "            futures.append(future)\n",
        "        # process all results\n",
        "        for future in as_completed(futures):\n",
        "            # open the file and load the data\n",
        "            contents, _ = future.result()\n",
        "            docs.extend(contents)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def split_documents(documents: list[Document]) -> tuple[list[Document], list[Document]]:\n",
        "    # Splits documents for correct Text Splitter\n",
        "    text_docs, python_docs = [], []\n",
        "    for doc in documents:\n",
        "        file_extension = os.path.splitext(doc.metadata[\"source\"])[1]\n",
        "        if file_extension == \".py\":\n",
        "            python_docs.append(doc)\n",
        "        else:\n",
        "            text_docs.append(doc)\n",
        "\n",
        "    return text_docs, python_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hz961E-DJ_if"
      },
      "outputs": [],
      "source": [
        "# def main():\n",
        "#     # Load documents and split in chunks\n",
        "#     logging.info(f\"Loading documents from {SOURCE_DIRECTORY}\")\n",
        "#     documents = load_documents(SOURCE_DIRECTORY)\n",
        "#     text_documents, python_documents = split_documents(documents)\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "#     python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "#         language=Language.PYTHON, chunk_size=1000, chunk_overlap=200\n",
        "#     )\n",
        "#     texts = text_splitter.split_documents(text_documents)\n",
        "#     texts.extend(python_splitter.split_documents(python_documents))\n",
        "#     logging.info(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")\n",
        "#     logging.info(f\"Split into {len(texts)} chunks of text\")\n",
        "\n",
        "#     # Create embeddings\n",
        "#     embeddings = HuggingFaceInstructEmbeddings(\n",
        "#         model_name=EMBEDDING_MODEL_NAME,\n",
        "#         model_kwargs={\"device\": \"cuda\"},  # Set device type to \"tpu\"\n",
        "#     )\n",
        "\n",
        "#     db = Chroma.from_documents(\n",
        "#         texts,\n",
        "#         embeddings,\n",
        "#         persist_directory=PERSIST_DIRECTORY,\n",
        "#         client_settings=CHROMA_SETTINGS,\n",
        "#     )\n",
        "#     db.persist()\n",
        "#     db = None\n",
        "\n",
        "# main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LkI52w1TUbh2"
      },
      "outputs": [],
      "source": [
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.llms import HuggingFacePipeline, LlamaCpp\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    GenerationConfig,\n",
        "    LlamaForCausalLM,\n",
        "    LlamaTokenizer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "def load_model(device_type, model_id, model_basename=None):\n",
        "    \"\"\"\n",
        "    Select a model for text generation using the HuggingFace library.\n",
        "    If you are running this for the first time, it will download a model for you.\n",
        "    subsequent runs will use the model from the disk.\n",
        "\n",
        "    Args:\n",
        "        device_type (str): Type of device to use, e.g., \"cuda\" for GPU or \"cpu\" for CPU.\n",
        "        model_id (str): Identifier of the model to load from HuggingFace's model hub.\n",
        "        model_basename (str, optional): Basename of the model if using quantized models.\n",
        "            Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        HuggingFacePipeline: A pipeline object for text generation using the loaded model.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If an unsupported model or device type is provided.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n",
        "    logging.info(\"This action can take a few minutes!\")\n",
        "\n",
        "    if model_basename is not None:\n",
        "        if \".ggml\" in model_basename:\n",
        "            logging.info(\"Using Llamacpp for GGML quantized models\")\n",
        "            model_path = hf_hub_download(repo_id=model_id, filename=model_basename)\n",
        "            max_ctx_size = 4000\n",
        "            kwargs = {\n",
        "                \"model_path\": model_path,\n",
        "                \"n_ctx\": max_ctx_size,\n",
        "                \"max_tokens\": max_ctx_size,\n",
        "            }\n",
        "            if device_type.lower() == \"mps\":\n",
        "                kwargs[\"n_gpu_layers\"] = 1000\n",
        "            if device_type.lower() == \"cuda\":\n",
        "                kwargs[\"n_gpu_layers\"] = 1000\n",
        "                kwargs[\"n_batch\"] = max_ctx_size\n",
        "            return LlamaCpp(**kwargs)\n",
        "\n",
        "        else:\n",
        "            # The code supports all huggingface models that ends with GPTQ and have some variation\n",
        "            # of .no-act.order or .safetensors in their HF repo.\n",
        "            logging.info(\"Using AutoGPTQForCausalLM for quantized models\")\n",
        "\n",
        "            if \".safetensors\" in model_basename:\n",
        "                # Remove the \".safetensors\" ending if present\n",
        "                model_basename = model_basename.replace(\".safetensors\", \"\")\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "            logging.info(\"Tokenizer loaded\")\n",
        "\n",
        "            model = AutoGPTQForCausalLM.from_quantized(\n",
        "                model_id,\n",
        "                model_basename=model_basename,\n",
        "                use_safetensors=True,\n",
        "                trust_remote_code=True,\n",
        "                device=\"cuda:0\",\n",
        "                use_triton=False,\n",
        "                quantize_config=None,\n",
        "            )\n",
        "    elif (\n",
        "        device_type.lower() == \"cuda\"\n",
        "    ):  # The code supports all huggingface models that ends with -HF or which have a .bin\n",
        "        # file in their HF repo.\n",
        "        logging.info(\"Using AutoModelForCausalLM for full models\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        logging.info(\"Tokenizer loaded\")\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "            # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
        "        )\n",
        "        model.tie_weights()\n",
        "    else:\n",
        "        logging.info(\"Using LlamaTokenizer\")\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
        "        model = LlamaForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "    # Load configuration from the model to avoid warnings\n",
        "    generation_config = GenerationConfig.from_pretrained(model_id)\n",
        "\n",
        "    # Create a pipeline for text generation\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=4000,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.15,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    logging.info(\"Local LLM Loaded\")\n",
        "\n",
        "    return local_llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mM5qrL7UZZI",
        "outputId": "ec6a7d1a-69cb-494b-da4e-05b5e4c8712f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load INSTRUCTOR_Transformer\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This function implements the information retrieval task.\n",
        "\n",
        "\n",
        "1. Loads an embedding model, can be HuggingFaceInstructEmbeddings or HuggingFaceEmbeddings\n",
        "2. Loads the existing vectorestore that was created by inget.py\n",
        "3. Loads the local LLM using load_model function - You can now set different LLMs.\n",
        "4. Setup the Question Answer retreival chain.\n",
        "5. Question answers.\n",
        "\"\"\"\n",
        "\n",
        "logging.info(f\"Running on: cuda\")\n",
        "# logging.info(f\"Running on: cpu\")\n",
        "\n",
        "# logging.info(f\"Display Source Documents set to: {show_sources}\")\n",
        "\n",
        "# embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cpu\"})\n",
        "embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"cuda\"})\n",
        "\n",
        "# load the vectorstore\n",
        "db = Chroma(\n",
        "    persist_directory=PERSIST_DIRECTORY,\n",
        "    embedding_function=embeddings,\n",
        "    client_settings=CHROMA_SETTINGS,\n",
        ")\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
        "just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "{history}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=template)\n",
        "memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n",
        "\n",
        "# llm = load_model(\"cuda\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
        "llm = load_model(\"cpu\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
        ")\n",
        "# Interactive questions and answers\n",
        "\n",
        "# while True:\n",
        "#     # query = input(\"\\nEnter a query: \")\n",
        "#     if query == \"exit\":\n",
        "#         break\n",
        "#     # Get the answer from the chain\n",
        "#     res = qa(query)\n",
        "#     answer, docs = res[\"result\"], res[\"source_documents\"]\n",
        "\n",
        "#     # Print the result\n",
        "#     print(\"\\n\\n> Question:\")\n",
        "#     print(query)\n",
        "#     print(\"\\n> Answer:\")\n",
        "#     print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3mMDcHWC5X"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from fastapi.responses import JSONResponse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# from flask import Flask, jsonify\n",
        "\n",
        "def recommendProduct(sql):\n",
        "    # Read the product dataset\n",
        "    productDf = pd.read_excel('/content/flipkartProductDataset.csv')\n",
        "    preprocessedSql = sql.lower()\n",
        "    preprocessedProducts = productDf.applymap(\n",
        "        lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(preprocessedSql)\n",
        "\n",
        "    # Perform part-of-speech tagging\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    # Find adjectives and nouns and combine them\n",
        "    adjective_noun_pairs = []\n",
        "    i = 0\n",
        "    while i != len(pos_tags):\n",
        "        if pos_tags[i][1].startswith('JJ'):\n",
        "            j = i\n",
        "            while not pos_tags[j][1].startswith('NN'):\n",
        "                j += 1\n",
        "\n",
        "            r = []\n",
        "            for k in range(i, j + 1):\n",
        "                if pos_tags[k][1].startswith('JJ') or pos_tags[k][1].startswith('RB') or pos_tags[k][1].startswith('NN'):\n",
        "                    r.append(pos_tags[k][0])\n",
        "            adjective_noun_pairs.append(r)\n",
        "            i = j + 1\n",
        "\n",
        "        elif pos_tags[i][1].startswith('NN'):\n",
        "            adjective_noun_pairs.append([pos_tags[i][0]])\n",
        "            i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # Create TF-IDF vectorizer\n",
        "    # preprocessedProducts['Combined'] = preprocessedProducts['product_category_tree'].str.cat(\n",
        "    #     preprocessedProducts['product_specifications'], sep=' ')\n",
        "\n",
        "    # Prepare the list of recommended products\n",
        "    recommended_products = []\n",
        "    for pair in adjective_noun_pairs:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        productVectors = vectorizer.fit_transform(preprocessedProducts['Features'].fillna(''))\n",
        "        query_vector = vectorizer.transform([\" \".join(pair)])\n",
        "        similarity_scores = cosine_similarity(query_vector, productVectors).flatten()\n",
        "\n",
        "        ranked_indices = similarity_scores.argsort()[::-1]\n",
        "        # ranked_products = preprocessedProducts.iloc[ranked_indices]\n",
        "        ranked_products = productDf.iloc[ranked_indices]\n",
        "\n",
        "        recommended_urls = ranked_products['pdt_url'].tolist()\n",
        "        recommended_prices = ranked_products['price'].tolist()\n",
        "        recommended_names = ranked_products['pdt_name'].tolist()\n",
        "        # recommended_img_url = ranked_products['image'].tolist()\n",
        "        #print(recommended_img_url)\n",
        "        recommendations = []\n",
        "        recommendations.extend(list(zip(recommended_urls[:10], recommended_prices[:10], recommended_names[:10])))\n",
        "\n",
        "        recommended_products.extend(recommendations)\n",
        "\n",
        "    recommended_products_list = []\n",
        "    for url, price, name in recommended_products:\n",
        "        recommended_product_dict = {\n",
        "            'name': name,\n",
        "            'url': url,\n",
        "            'price': price\n",
        "        }\n",
        "\n",
        "        recommended_products_list.append(recommended_product_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #print((recommended_img_url[0].split(',')[0].replace(\"[\",\"\")).replace('\"',\"\"))\n",
        "    #final_dict=[]\n",
        "    # return jsonify(recommended_products_list)\n",
        "    # return JSONResponse(content=recommended_products_list)\n",
        "    return recommended_products_list[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLKRT2vr-7tT"
      },
      "outputs": [],
      "source": [
        "from chromadb.api.types import QueryResult\n",
        "from fastapi import Request\n",
        "from fastapi.responses import JSONResponse\n",
        "# Define your endpoint for handling prompts\n",
        "@app.post('/prompt')\n",
        "async def handle_prompt(request:Request):\n",
        "    try:\n",
        "        # with app.app_context():\n",
        "        data = await request.json()\n",
        "        query = data['query']\n",
        "        # products=data['prod']\n",
        "        # products=\"kurta,pajama,dhoti,suit,shirt,pant,trouser,blazer\"\n",
        "        print(query)\n",
        "        if not query:\n",
        "            return JSONResponse(content={\"error\": \"Missing 'query' parameter\"}, status_code=400)\n",
        "\n",
        "\n",
        "        res = qa(query)\n",
        "        answer = res['result']\n",
        "        print(answer)\n",
        "        responseToSend = recommendProduct(answer)\n",
        "\n",
        "        return JSONResponse(content={\"question\": query, \"answer\": responseToSend})\n",
        "    except Exception as e:\n",
        "        logging.error(e)\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcYzPRJch3tb"
      },
      "outputs": [],
      "source": [
        "# Use ngrok to expose the local Flask app to a public URL\n",
        "ngrok_tunnel=ngrok.connect(3000)\n",
        "# public_url = ngrok.connect(addr=\"3000\", proto=\"http\")\n",
        "\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app,port=3000)\n",
        "# if __name__ == '__main__':\n",
        "    # app.run(host='0.0.0.0', port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
